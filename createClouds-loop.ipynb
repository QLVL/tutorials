{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The full python workflow for semasiological token-level clouds on a loop\n",
    "\n",
    "This notebook reproduces the code in the [single-lemma workflow](createClouds.ipynb) but on a loop across multiple lemmas, as it was run for [*Cloudspotting. Visual Analytics for Distributional Semantics*](https://cloudspotting.marianamontes.me/). The cloud is not run here, only shown with dummy file paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initial setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pandas as pd\n",
    "sys.path.append('/path/to/nephosem/') # path to the nephosem repository\n",
    "sys.path.append('path/to/scripts/')# path to semasioFlow repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from semasioFlow import ConfigLoader\n",
    "from semasioFlow.load import loadVocab, loadMacro, loadColloc, loadFocRegisters\n",
    "from semasioFlow.sample import sampleTypes\n",
    "from semasioFlow.focmodels import createBow, createRel, createPath\n",
    "from semasioFlow.socmodels import targetPPMI, weightTokens, createSoc\n",
    "from semasioFlow.utils import plotPatterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on what you need, you will have to set up some useful paths settings.\n",
    "I like to have at least the path to my project (`mydir`), an output path within (`mydir + \"output\"`) and a GitHub path for the datasets that I will use in the visualization. There is no real reason not to have everything together, except that I did not think of it at the moment. (Actually, there is: the GitHub stuff will be public and huge data would not be included. How much do we want to have public?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydir = \"../\"\n",
    "output_path = f\"{mydir}/output/\"\n",
    "nephovis_path = f\"{mydir}/for-nephovis/\"logging.basicConfig(filename = f'{mydir}/mylog.log', filemode = 'w', level = logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_subfolders = ['vocab', 'cws', 'registers', 'tokens']\n",
    "for sf in necessary_subfolders:\n",
    "    if not os.path.exists(output_path + sf):\n",
    "        os.makedirs(output_path + sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables with paths is just meant to make it easier to manipulate filenames. The most important concrete step is to adapt the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = ConfigLoader()\n",
    "default_settings = conf.settings\n",
    "conf = ConfigLoader()\n",
    "settings = conf.update_config('config.ini')\n",
    "settings['output-path'] = output_path\n",
    "corpus_name = 'corpus_name'\n",
    "\n",
    "print(settings['line-machine'])\n",
    "print(settings['global-columns'])\n",
    "print(settings['type'], settings['colloc'], settings['token'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Frequency lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency lists are the first thing to create, but once you have them, you just load them. So what we are going to do here is define the filename where we *would* store the frequency list (in this case, where it is actually stored), and if it exists it loads it; if it doesn't, it creates and store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_name = f\"{output_path}/vocab/{corpus_name}.nodefreq\"\n",
    "print(full_name)\n",
    "full = loadVocab(full_name, settings)\n",
    "full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Boolean token-level matrices\n",
    "\n",
    "Even though we first think of the type leven and only afterwards of the token level, with this workflow we don't really need to touch type level until after we obtain the boolean token-level matrices, that is, until we need to use PPMI values to select or weight the context words.\n",
    "\n",
    "As a first step, we need the type or list of types we want to run; for example `\"heet/adj\"` or `[\"vernietig/verb\", \"verniel/verb\"]`, and we subset the vocabulary for that query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = f\"{mydir}/sources/listofnames.fnames\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noun_lemmas = ['horde', 'hoop', 'spot', 'staal', 'stof', 'schaal', 'blik']\n",
    "adj_lemmas = ['heilzaam', 'hoekig', 'gekleurd', 'dof', 'hachelijk', 'geestig', 'hoopvol',\n",
    "              'hemels', 'geldig', 'gemeen', 'goedkoop', 'grijs', 'heet']\n",
    "verb_lemmas = ['herroepen', 'heffen', 'huldigen', 'haten', 'herhalen', 'herinneren',\n",
    "              'diskwalificeren', 'harden', 'herstellen', 'helpen', 'haken', 'herstructureren']\n",
    "verb_stems = ['herroep', 'hef', 'huldig', 'haat', 'herhaal', 'herinner',\n",
    "              'diskwalificeer', 'hard', 'herstel', 'help', 'haak', 'herstructureer']\n",
    "only_nouns = [(x, [x+'/noun']) for x in noun_lemmas]\n",
    "only_adjs = [(x, [x+'/adj']) for x in adj_lemmas]\n",
    "only_verbs = [(x, [y+'/verb']) for x, y in zip(verb_lemmas, verb_stems)]\n",
    "everything = only_nouns + only_adjs + only_verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could generate the tokens for all 10k tokens, or create a random selection with a certain number and then only use those files. The output of sampleTypes includes a list of token IDs as well as the list of filenames that suffices to extract those tokens. We can then use the new list of filenames when we collect tokens, and the list of tokens to subset the resulting matrices.\n",
    "\n",
    "Of course, to keep the sample fixed it would be more useful to generate the list, store it and then retrieve it in future runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path\n",
    "\n",
    "with open(f\"{mydir}/sources/adjIds.txt\", 'r') as f1:\n",
    "    adjs = [x.strip() for x in f1.readlines()]\n",
    "with open(f\"{mydir}/sources/nounIds.txt\", 'r') as f2:\n",
    "    nouns = [x.strip() for x in f2.readlines()]\n",
    "with open(f\"{mydir}/sources/verbIds.txt\", 'r') as f3:\n",
    "    verbs = [x.strip() for x in f3.readlines()]\n",
    "tokenlist = adjs + nouns + verbs\n",
    "\n",
    "# 3. Extract filenames from token ID's and map to paths ================================\n",
    "token2fname = [x.split('/')[2]+'.conll' for x in tokenlist]\n",
    "with open(fnames, 'r') as q:\n",
    "    fnameSample = [x.strip() for x in q.readlines() if x.strip().rsplit('/', 1)[1] in token2fname]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_pos = [x for x in foc.get_item_list() if x.split(\"/\")[1] in [\"noun\", \"adj\", \"verb\", \"adv\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foc_win = [(3, 3), (5, 5), (10, 10)] #three options of symmetric windows with 3, 5 or 10 words to each side\n",
    "foc_pos = {\n",
    "    \"all\" : foc.get_item_list(), # the filter has already been applied in the FOC list\n",
    "    \"lex\" : lex_pos # further filter by part-of-speech\n",
    "}\n",
    "bound = { \"match\" : \"^</sentence>$\", \"values\" : [True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. On a loop per item, row create Bow ================================\n",
    "for type_name, query_list in everything:\n",
    "    query = full.subvocab(query_list)\n",
    "    bowdata = createBow(query, settings, type_name = type_name, fnames = fnameSample, tokenlist = tokenlist,\n",
    "         foc_win = foc_win, foc_pos = foc_pos, bound = bound)\n",
    "    \n",
    "    # 5. Most probably, store register ================================\n",
    "    models_fname = f\"{output_path}/registers/{type_name}.bow-models.tsv\"\n",
    "    bowdata.to_csv(models_fname, sep=\"\\t\", index_label = '_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Lemmarel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings['separator-line-machine'] = \"^</sentence>$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. On a loop per item, row create Path ================================\n",
    "graphml_name = \"LEMMAREL.verbs\"\n",
    "templates_dir = f\"{mydir}/templates\"\n",
    "rel_macros = [\n",
    "    (\"LEMMAREL1\", loadMacro(templates_dir, graphml_name, \"LEMMAREL1.verbs\")),\n",
    "    (\"LEMMAREL2\", loadMacro(templates_dir, graphml_name, \"LEMMAREL2.verbs\"))\n",
    "]\n",
    "for type_name, query_list in only_verbs:\n",
    "    query = full.subvocab(query_list)\n",
    "    sub_tokenlist = [x for x in tokenlist if x.startswith(query_list[0])]\n",
    "    sub_fnameSample = [x for x in fnameSample if x.rsplit(\"/\", 1)[1] in [y.split(\"/\")[2]+'.conll' for y in sub_tokenlist]]\n",
    "    print(query_list[0])\n",
    "    reldata = createRel(query, settings, rel_macros, type_name = type_name,\n",
    "                        fnames = sub_fnameSample, tokenlist = sub_tokenlist, foc_filter = foc.get_item_list())\n",
    "    \n",
    "    # 7. Most probably, store register ================================\n",
    "    models_fname = f\"{output_path}/registers/{type_name}.rel-models.tsv\"\n",
    "    reldata.to_csv(models_fname, sep=\"\\t\", index_label = '_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 6. On a loop per item, row create Path ================================\n",
    "graphml_name = \"LEMMAREL.nouns\"\n",
    "templates_dir = f\"{mydir}/templates\"\n",
    "rel_macros = [\n",
    "    (\"LEMMAREL1\", loadMacro(templates_dir, graphml_name, \"LEMMAREL1.nouns\")),\n",
    "    (\"LEMMAREL2\", loadMacro(templates_dir, graphml_name, \"LEMMAREL2.nouns\")),\n",
    "    (\"LEMMAREL3\", loadMacro(templates_dir, graphml_name, \"LEMMAREL3.nouns\"))\n",
    "]\n",
    "for type_name, query_list in only_nouns:\n",
    "    query = full.subvocab(query_list)\n",
    "    sub_tokenlist = [x for x in tokenlist if x.startswith(query_list[0])]\n",
    "    sub_fnameSample = [x for x in fnameSample if x.rsplit(\"/\", 1)[1] in [y.split(\"/\")[2]+'.conll' for y in sub_tokenlist]]\n",
    "    print(query_list[0])\n",
    "    reldata = createRel(query, settings, rel_macros, type_name = type_name,\n",
    "                        fnames = sub_fnameSample, tokenlist = sub_tokenlist, foc_filter = foc.get_item_list())\n",
    "    \n",
    "    # 7. Most probably, store register ================================\n",
    "    models_fname = f\"{output_path}/registers/{type_name}.rel-models.tsv\"\n",
    "    reldata.to_csv(models_fname, sep=\"\\t\", index_label = '_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 6. On a loop per item, row create Path ================================\n",
    "graphml_name = \"LEMMAREL.adjs\"\n",
    "templates_dir = f\"{mydir}/templates\"\n",
    "rel_macros = [\n",
    "    (\"LEMMAREL1\", loadMacro(templates_dir, graphml_name, \"LEMMAREL1.adjs\")),\n",
    "    (\"LEMMAREL2\", loadMacro(templates_dir, graphml_name, \"LEMMAREL2.adjs\"))\n",
    "]\n",
    "for type_name, query_list in only_adjs:\n",
    "    query = full.subvocab(query_list)\n",
    "    sub_tokenlist = [x for x in tokenlist if x.startswith(query_list[0])]\n",
    "    sub_fnameSample = [x for x in fnameSample if x.rsplit(\"/\", 1)[1] in [y.split(\"/\")[2]+'.conll' for y in sub_tokenlist]]\n",
    "    reldata = createRel(query, settings, rel_macros, type_name = type_name,\n",
    "                        fnames = sub_fnameSample, tokenlist = sub_tokenlist, foc_filter = foc.get_item_list())\n",
    "    \n",
    "    # 7. Most probably, store register ================================\n",
    "    models_fname = f\"{output_path}/registers/{type_name}.rel-models.tsv\"\n",
    "    reldata.to_csv(models_fname, sep=\"\\t\", index_label = '_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Lemmapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphml_name = \"LEMMAPATH\"\n",
    "templates_dir = f\"{mydir}/templates\"\n",
    "path_templates = [loadMacro(templates_dir, graphml_name, f\"LEMMAPATH{i}\") for i in [1, 2, 3]]\n",
    "path_macros = [\n",
    "    # First group includes templates with one and two steps, no weight\n",
    "    (\"LEMMAPATH2\", [path_templates[0], path_templates[1]], None),\n",
    "    # Second group includes templates with up to three steps, no weight\n",
    "    (\"LEMMAPATH3\", [path_templates[0], path_templates[1], path_templates[2]], None),\n",
    "    # Third group includes templates with up to three steps, with weight\n",
    "    (\"LEMMAPATHweight\", [path_templates[0], path_templates[1], path_templates[2]], [1, 0.6, 0.3])\n",
    "]\n",
    "settings['separator-line-machine'] = \"^</sentence>$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. On a loop per item, row create Path ================================\n",
    "for type_name, query_list in everything:\n",
    "    query = full.subvocab(query_list)\n",
    "    sub_tokenlist = [x for x in tokenlist if x.startswith(query_list[0])]\n",
    "    sub_fnameSample = [x for x in fnameSample if x.rsplit(\"/\", 1)[1] in [y.split(\"/\")[2]+'.conll' for y in sub_tokenlist]]\n",
    "    pathdata = createPath(query, settings, path_macros, type_name = type_name,\n",
    "          fnames = sub_fnameSample, tokenlist = sub_tokenlist, foc_filter = foc.get_item_list())\n",
    "    \n",
    "    # 9. Most probably, store register ================================\n",
    "    models_fname = f\"{output_path}/registers/{type_name}.path-models.tsv\"\n",
    "    pathdata.to_csv(models_fname, sep=\"\\t\", index_label = '_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Weight or booleanize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Create/load collocation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldir = \"/path/to/dataframes/\"\n",
    "freq_fname_CW4 = f\"{coldir}/{corpus_name}.fullcorpus_CW4.wcmx.freq.pac\" # window size of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings['left-span'] = 4\n",
    "settings['right-span = 4']\n",
    "freqMTX_CW4 = loadColloc(freq_fname_CW4, settings, row_vocab = full)\n",
    "freqMTX_CW4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_fname_CW10 = f\"{coldir}/{corpus_name}.fullcorpus_CW10.wcmx.freq.pac\" # window size of 10\n",
    "settings['left-span'] = 10\n",
    "settings['right-span = 10']\n",
    "freqMTX_CW10 = loadColloc(freq_fname_CW10, settings, row_vocab = full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Register PPMI values\n",
    "(Done with 4.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Implement weighting on selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semasioFlow.utils import booleanize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. On a loop per item, row weight models ================================\n",
    "for type_name, query_list in everything:\n",
    "    nephovis_type = f\"{nephovis_path}/{type_name}\"\n",
    "    ppmi = targetPPMI(query_list, vocabs = {\"freq\" : full},\n",
    "               collocs = {\"4\" : freqMTX_CW4, \"10\" : freqMTX_CW10},\n",
    "               type_name = type_name, output_dir = f\"{nephovis_type}/\",\n",
    "               main_matrix = \"4\")\n",
    "    weighting = {\n",
    "        \"no\" : None,\n",
    "        \"selection\" : booleanize(ppmi, include_negative=False),\n",
    "        \"weight\" : ppmi\n",
    "    }\n",
    "    token_dir = f\"{output_path}/tokens/{type_name}\"\n",
    "    foc_registers = loadFocRegisters(f\"{output_path}/registers\", type_name)\n",
    "    weight_data = weightTokens(token_dir, weighting, foc_registers)\n",
    "    weight_data[\"model_register\"].to_csv(f\"{output_path}/registers/{type_name}.focmodels.tsv\", sep = '\\t',\n",
    "                                         index_label = \"_model\")\n",
    "    weight_data[\"token_register\"].to_csv(f\"{nephovis_type}/{type_name}.variables.tsv\", sep = '\\t',\n",
    "                                         index_label = \"_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Second-order dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soc_pos = {\n",
    "    \"all\" : selection_without_filters,\n",
    "    \"nav\" : special_selection\n",
    "}\n",
    "lengths = [\"FOC\", 5000] # a number will take the most frequent; something else will take the FOC items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. On a loop per item, create Soc models ================================\n",
    "for type_name, query_list in everything:\n",
    "    registers = pd.read_csv(f\"{output_path}/registers/{type_name}.focmodels.tsv\",\n",
    "                            sep = \"\\t\", index_col = \"_model\")\n",
    "    token_dir = f\"{output_path}/tokens/{type_name}\"\n",
    "    socdata = createSoc(token_dir, registers = registers,\n",
    "                        soc_pos = soc_pos, lengths = lengths,\n",
    "                        socMTX = freqMTX_CW4, store_focdists = f\"{output_path}/cws/{type_name}/\")\n",
    "    socdata.to_csv(f\"{nephovis_path}/{type_name}/{type_name}.models.tsv\", sep = \"\\t\", index_label=\"_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Cosine distances\n",
    "Once we have all the token-level vectors, as well as our registers,\n",
    "we can quickly compute and store their cosine distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nephosem import TypeTokenMatrix\n",
    "from nephosem.specutils.mxcalc import compute_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. On a loop per item, compute distances ======================================\n",
    "input_suffix = \".tcmx.soc.pac\" #token by context matrix\n",
    "output_suffix = \".ttmx.dist.pac\" # token by token matrix\n",
    "for type_name, query_list in everything:\n",
    "    token_dir = f\"{output_path}/tokens/{type_name}\"\n",
    "    socdata = pd.read_csv(f\"{github_dir}/{type_name}/{type_name}.models.tsv\",\n",
    "                         sep = \"\\t\", index_col = \"_model\")\n",
    "    for modelname in socdata.index:\n",
    "        input_name = f\"{token_dir}/{modelname}{input_suffix}\"\n",
    "        output_name = f\"{token_dir}/{modelname}{output_suffix}\"\n",
    "        compute_distance(TypeTokenMatrix.load(input_name)).save(output_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: context word detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semasioFlow.contextwords import listContextwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # On a loop\n",
    "for type_name, query_list in everything:\n",
    "    cws = listContextwords(type_name, tokenlist, fnameSample, settings, left_win=15, right_win = 15)\n",
    "    cw_fname = f\"{nephovis_path}/{type_name}/{type_name}.cws.detail.tsv\"\n",
    "    cws.to_csv(cw_fname, sep = \"\\t\", index_label = \"cw_id\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
